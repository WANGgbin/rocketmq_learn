介绍 rocketmq 中 consumer 相关逻辑。

几个问题：

- 消费者的信息如何管理
- 消费者的 rebalance 流程
- 消费模型是怎么样的、如何提高消费的性能、如何做流控
- 消费者的重试逻辑是怎么样的
- 顺序消费是什么？如何实现的？

# rebalance

在集群模式下，一个 topic 下的一个 queue 只能分配给同一个消费组下的一个消费者。当消费者变化(上下线、异常宕机)、topic queue 发生变化的时候，
就需要进行重新分配，而这个重新分配的流程就是 rebalance。

rmq 中 rebalance 的时机如下：
- 消费者上下线
- topic queue 数量发生变化
- 消费者定时执行 rebalance

与 kafka 相比，rmq 中的 rebalance 流程相对简单，注意 rebalance 流程是在消费者端进行的，流程如下：
- 获取 topic 所有 queue 信息
- 从 broker(topic 下queue 所在的 主 broker，随便选一个) 获取消费组下所有消费者信息
- 按照指定的分配算法(avg、consistent hash) 获取当前消费者队列信息
- 之前在现在不在的队列，不再拉取消息，同时提交消费偏移量
- 新加入的队列，从 broker(queue 对应的主 broker 节点)获取消费偏移量，拉取消息，进行消费。

kafka 中分配工作是交给一个消费者完成，然后由其他消费者同步分配结果。但是在 rmq 中，每个消费者
自己计算自己要消费的目标队列。而保证各个消费者分配结果不重不漏，就是各个节点获取相同的信息、执行相同的分配算法，保证结果一致性。

rebalance 缺点如下：
- 重复消费

    旧消费者的消费偏移量如何异步提交，重平衡的时候，最新的偏移量可能还未提交。接管该队列的消费者就从旧的偏移量开始消费数据，导致重复消费。

- 停止消费

    当某个队列在消费者之间转移的时候，旧的消费者停止消费。只有转移到新的消费者后，才进行消费。

# 消费模型

为了保证吞吐，整个消费模型是个生产者、消费者的模型。生产者负责从 broker 拉取消息，拉取消息后扔到队列中，消费者(线程池)从队列中获取消息消费。

## 生产者

每个消费者通过 rebalance 确定自己要消费的 queue，对于每个 queue，在消费者端，这个 queue 对应两个对象 messageQueue、produceQueue。

messageQueue 存放 topic queue 的原信息，包括 topic、queueID 等。
produceQueue 存放从 broker 拉取的消息，需要注意的是：消息存放结构并不是一个简单的队列，而是一个红黑树，目的是按照消息的偏移量排序。这样在
某个消息消费后，通过获取树中第一个元素的偏移量，即可确定要提交的偏移量。

生产者又是如何从 queue 拉取消息的呢？

rmq 也是通过生产者、消费者模式实现的。当 rebalance 确定要消费的 queue 后，对于每个 queue 会封装一个 pullRequest(pullRequest 中存储拉取消息的偏移量)
扔到一个队列中，然后 生产者消费 pullRequest 从 broker 获取消息。当生产者消费 pullRequest 后，又会将 pullRequest(根据本次拉取的消息更新下次拉取的偏移量)
回灌到队列中，这样就可以实现不断的从一个 queue 中拉取消息。

我们知道消费者消费消息有两种模式 pull 和 push，实际上 push 模式底层也是通过消费者不断从 broker 拉取消息完成，只不过拉取间隔是 0，即拉取完一批消息
后，会立刻回灌 pullRequest 从而立即再次从 queue 拉取消息。

rmq consumer 为啥不一个 queue 一个线程拉取消息呢？这种方式有以下问题：
- 线程可能会很多，浪费系统资源，还会导致线程上下文切换
- 当 queue 没消息或者同步拉取的时候，线程闲置，线程利用率低。
- 扩展性差，queue 变换时，线程也要同步销毁或者创建。

而通过生产者-消费者模型。要从某个 queue 拉取消息，只需要创建 pullRequest 即可，扩展性好。此外，通过线程池消费模式(似乎只有一个线程)能够控制
线程数量，同时每个线程通过异步的方式从 broker 拉取消息，线程利用率更高。

如果生产者从 broker 拉取消息很快，而消费者消费速度很慢，会不会导致缓存消息的内存爆炸。

实际上不会的，rmq consumer 端是有背压(flow control) 策略的。每次消费 pullRequest 的时候，生产者都会判断队列中积压的数据数量或者数据大小
是否达到一定值，如果达到一定值则本次不从 broker 拉取消息，并延时投递 pullRequest.

此外，当 broker 没有消息的时候，不断的轮询会造成资源的浪费。rmq 的解决办法是长轮询，即没有数据的时候，多等一会儿再返回。

另外一个比较有意思的是，我们知道 broker 是主从模式的，那么应该从哪个 master 还是 slave 拉取消息呢？

rmq 的解决思路是，每次从 broker 拉取消息时，broker 根据拉取的偏移量决定下次拉取从 master 还是 slave 拉取：
- 如果拉取的偏移量在 master page cache 中，则从 master 拉取，这样很快。
- 否则为了降低 master 的压力，比较久的数据从 slave 拉取。

## 消费者

无论是并发消费还是顺序消费，消费者都是个线程池。当生产者获取到消息后，会将本次获取到的消息封装为 consumerRequest 扔到队列中，线程池消费
consumerRequest 完成消息的消费。

当成功消费消息后，会清除 produceQueue 中的消息并更新消费偏移量，通过定时任务持久化到 broker。

不过并发消费和顺序消费的具体实现还是不一样的。顺序消费为了保证一个消费组中，一个 queue 只有一个线程消费，实现更复杂些。

# 顺序消费

顺序消费指的是在一个消费组中，topic 的某一个 queue，任意时刻只能由一个线程消费。

那么 rmq 是如何实现顺序消费的呢？

首先我们需要确保：某个 queue 只能属于某一个 consumer。实现的方式就是分布式锁。在顺序消费场景下，在 consumer
rebalance 的时候，对于分配的每个队列，都会向队列所在的 master broker 发起锁请求(就是分布式锁)，只有占有分布式
锁成功。consumer 才会消费该 queue。占有分布式锁后，同样需要锁续期，因此 consumer 还会有个定时任务执行锁续期任务。

要实现顺序消费，我们还需要在一个 consumer 中，只有一个线程消费某个 queue。为此，consumer 端为每个 messageQueue
维护了一个锁，当从 queue 拉取到消息后，会封装成 consumeRequest 并提交给线程池，而 consumeRequest 的 run 方法中
就会对 messageQueue 的 lock 对象执行 synchronized 操作，进而保证只有一个线程消费该 queue 里面的消息。

目前为止，似乎能保证顺序消费了。但是在 rebalance 场景下，我们说可能会有重复消费的问题。所以在重平衡期间，如果 consumer
需要移除某个消费者，需要确保当前消费者没有线程在消费这个队列的数据，这样才会执行队列移除(提交位移、删除分布式锁)。为此，
还会引入一个锁，在消费的时候会占有该锁，在 rebalance 移除队列时候也会占有该锁。

有了上面的 3 个锁，就可以实现顺序消费了。

## consumeRequest

在顺序消费模式下，从 broker 拉取到消息后，只有当前没有线程消费该队列消息，才会创建一个 consumeRequest 否则并不会
创建 consumeRequest。

当线程消费 consumeRequest 时，会不断从 produceQueue 中拉取消息消费。当然，为了避免一个 queue 中的数据一直占有
线程资源，当线程消费达到一定时长的时候，会停止消费，并回灌 consumeRequest。从而完成 queue 的继续消费。

## 重试

顺序消费模式下，某个消息消费失败后，则不会继续消费后续的请求。而是不断重试当前的消息，直到成功或者达到最大消费次数。达到最大消费次数后，
会发送给 broker，broker 会将此消息存入到死信队列中。

# 并发消费

相对而言，并发消费简单很多。没有什么锁。

需要注意的是，一个 queue 可能有多个线程消费。可能存在的问题是，queue 后面的消息消费成功了，但是 queue 前面的消息有可能消费失败或者超时，
这导致对于已经消费成功的消息无法提交消费位移。

一种方案就是在消费端同步重试消费失败的消息，一直到成功或者达到重试上限，再提交位移。我们知道 consumer 消费的时候是有
flow control 策略的，其中一个就是如果 processQueue 中消息 span(第一条消息与最后一条消息的 offset 差值)过大，就会停止消费消息。因此，
如果消费一直失败，失败的消息会一直保存在 processQueue 中，进而导致 span 过大，进而导致消费停滞。

这就涉及到失败或者超时后，如何进行重试。在并发消费模式下，rmq 的处理思路是：把失败或者超时的消息从本地 processQueue 中移除，不阻塞位移提交，
但是失败的消息还需要重试，怎么办呢？回灌到重试的 topic 中。每一个 consumer group 都会有一个重试队列。

consumer 在启动的时候，还会订阅对应的消费者组的重试 topic 信息，因此拉取消息的时候，还会从重试 topic 拉取消息。

## consumeRequest

并发模式就会简单些，只要从 broker 拉取到消息，就会封装为一个 consumeRequest。因为不需要保证 queue 的顺序消费。

# 消费偏移量管理

消费者在消费一条消息后，如何提交位移呢？

有两种方式：
- 自动提交
- 手动提交

在集群模式下，消费组各个 queue 的消费进度是保存在 broker 的。如果每次提交都是保存到 broker 性能较差。

rmq 提交位移的思路如下：
1. 消费者客户端内存保存消费进度
2. 消费者客户端定时将消费进度保存到 broker
3. broker 将消费进度保存到内存
4. broker 定时将消费进度保存到磁盘

这种思路的优点是性能较好，保存都是在内存进行。通过定时任务完成持久化。

但问题就是消费进度的持久化是滞后的，这导致真正保存的消费进度小于实际的消费进度。从而导致重复消费的问题。
因此，在消费者端，一定要做好消息的幂等处理。

# 总结

整体消费逻辑如下：
1. rebalance 决定消费哪些 queue
2. producer 从 queue 拉取消息，将消息存储到 produceQueue 中，并封装 consumeRequest 扔到队列中
3. consumer 消费 consumeRequest 请求
4. 成功消费后，从 produceQueue 中删除消息，并更新偏移量
5. 通过定时任务将消费偏移量持久化到 broker
6. 如果消费失败，对于并发消费，则将消息以延时消息的形式回灌到 broker 后，时间到达后则投递到 consumer group 对应的重试队列，
消费者从该重试队列拉取消息，从而完成重试。对于顺序消费，为了保证顺序不能发送到 broker，只能在本地重试，达到重试上限后，回灌到
broker 的死信队列中。